{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from dfply import bind_rows\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', \n",
    "                 header=0, \n",
    "                 encoding=\"latin-1\", \n",
    "                 names=['class', 'text'], \n",
    "                 usecols=['class', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para pré-processamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PRE_PROCESS = True\n",
    "\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    symbols_pattern = r'[!\"#\\$%&\\'\\(\\)\\*\\+,\\-\\–\\/:=\\?@\\[\\\\\\]\\^_`{\\|}~º(...)]'\n",
    "    text = re.sub(symbols_pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    result = [token for token in tokens if token not in stop_words]\n",
    "    #return ' '.join(clean_text)\n",
    "    return result\n",
    "\n",
    "def stemmer(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in tokens]\n",
    "\n",
    "def lemmatizer(tokens):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if (INITIAL_PRE_PROCESS):\n",
    "        tokens = tokenize(text)\n",
    "        return tokens\n",
    "    else:\n",
    "        text = text_lowercase(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_punctuation(text)\n",
    "        tokens = tokenize(text)\n",
    "        #tokens = remove_stopwords(tokens)\n",
    "        tokens = lemmatizer(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_documents = df[df['class'] == 'ham'].copy()\n",
    "spam_documents = df[df['class'] == 'spam'].copy()\n",
    "\n",
    "ham_documents['out'] = 0\n",
    "spam_documents['out'] = 1\n",
    "\n",
    "ham_documents = ham_documents.drop(['class'], axis=1)\n",
    "spam_documents = spam_documents.drop(['class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(ham_documents, spam_documents):\n",
    "    total_documents = len(ham_documents) + len(spam_documents)\n",
    "    log_prior_ham = np.log(len(ham_documents) / total_documents)\n",
    "    log_prior_spam = np.log(len(spam_documents) / total_documents)\n",
    "\n",
    "    big_document = { 'ham': [], 'spam': [] }\n",
    "\n",
    "    big_document['ham'] = [word for document in ham_documents['text'] for word in preprocess_text(document)]\n",
    "    big_document['spam'] = [word for document in spam_documents['text'] for word in preprocess_text(document)]\n",
    "\n",
    "    vocabulary = set([word for document in [*ham_documents['text'], *spam_documents['text']] for word in preprocess_text(document)])\n",
    "\n",
    "    ham_counter = Counter(big_document['ham'])\n",
    "    spam_counter = Counter(big_document['spam'])\n",
    "\n",
    "    ham_denominator = sum([amount for (_, amount) in ham_counter.items()]) + len(vocabulary)\n",
    "    spam_denominator = sum([amount for (_, amount) in spam_counter.items()]) + len(vocabulary)\n",
    "\n",
    "    log_likelihood_ham = { word: np.log((ham_counter[word] + 1)/ham_denominator) for word in vocabulary }\n",
    "    log_likelihood_spam = { word: np.log((spam_counter[word] + 1)/spam_denominator) for word in vocabulary }\n",
    "    \n",
    "    return (log_prior_ham, log_prior_spam, log_likelihood_ham, log_likelihood_spam, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será considerado o encoding: 0 = ham, 1 = spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(document, model): \n",
    "    log_prior_ham, log_prior_spam, log_likelihood_ham, log_likelihood_spam, vocabulary = model\n",
    "    \n",
    "    ham_probability = log_prior_ham\n",
    "    spam_probability = log_prior_spam\n",
    "\n",
    "    for word in preprocess_text(document):\n",
    "        if word in vocabulary:\n",
    "            ham_probability += log_likelihood_ham[word]\n",
    "            spam_probability += log_likelihood_spam[word]\n",
    "\n",
    "\n",
    "    return np.argmax([ham_probability, spam_probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(ham_documents, spam_documents, k=10):\n",
    "    documents = ham_documents.append(spam_documents)\n",
    "    documents = documents.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    cut_size = round(len(documents) / k)\n",
    "    groups = [documents[index:index + cut_size] for index in range(0, len(documents), cut_size)]\n",
    "\n",
    "    if (len(groups) > k):\n",
    "        groups[-2] = groups[-2].append(groups[-1])\n",
    "\n",
    "    groups = groups[:k]\n",
    "    \n",
    "    for test_group_index in range(k):\n",
    "        train_documents = groups[:test_group_index] + groups[test_group_index+1:]\n",
    "        \n",
    "        flat_traindocs = pd.DataFrame()\n",
    "        \n",
    "        for document in train_documents:\n",
    "            if flat_traindocs.empty:\n",
    "                flat_traindocs = document\n",
    "            else: \n",
    "                flat_traindocs = flat_traindocs.append(document)\n",
    "        \n",
    "        test_documents = groups[test_group_index]\n",
    "        yield (flat_traindocs, test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(setup):\n",
    "    train_dataset, test_dataset = setup\n",
    "    \n",
    "    ham_train = train_dataset[train_dataset['out'] == 0]\n",
    "    spam_train = train_dataset[train_dataset['out'] == 1]\n",
    "\n",
    "    ham_test = test_dataset[test_dataset['out'] == 0]\n",
    "    spam_test = test_dataset[test_dataset['out'] == 1]\n",
    "\n",
    "    naive_bayes_classifier = fit(ham_train, spam_train)\n",
    "    \n",
    "    result = [(predict(document, naive_bayes_classifier), expected_outcome) for _,(document, expected_outcome) in test_dataset.iterrows()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(results):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for outcome, expected_outcome in results:\n",
    "        true_positives += outcome == 1 and expected_outcome == 1\n",
    "        true_negatives += outcome == 0 and expected_outcome == 0\n",
    "        false_positives += outcome == 1 and expected_outcome == 0\n",
    "        false_negatives += outcome == 0 and expected_outcome == 1\n",
    "        \n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics():\n",
    "    K = 10\n",
    "    setups = cross_validation(ham_documents, spam_documents, K)\n",
    "    sum_precision = 0\n",
    "    sum_recall = 0\n",
    "    sum_f1 = 0\n",
    "    \n",
    "    for setup in setups:\n",
    "        results = experiment(setup)\n",
    "        precision, recall, f1 = score(results)\n",
    "        sum_precision += precision\n",
    "        sum_recall += recall\n",
    "        sum_f1 += f1\n",
    "        #print(precision, recall, f1)\n",
    "        #print()\n",
    "        \n",
    "    print('average precision: %0.3f' % (sum_precision/K))\n",
    "    print('average recall: %0.3f' % (sum_recall/K))\n",
    "    print('average f1: %0.3f' %(sum_f1/K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes do modelo (inicial e melhorado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para um primeiro teste do modelo foi realizado o treinamento com um fluxo simples de pré-processamento, onde era realizada somente a tokenização das palavras. Para executar o modelo dessa forma foi criado o flag `INITIAL_PRE_PROCESS` que, quando receber o valor `True` realizará o fluxo simples citado anteriormente.\n",
    "\n",
    "Caso essa flag receba o valor `False` será realizado todo o pipeline de pré-processamento sobre o texto com as seguintes etapas:\n",
    "\n",
    "1. Conversão das palavras em minúsculo\n",
    "2. Remoção de números\n",
    "3. Remoção de pontuação\n",
    "4. Tokenização\n",
    "5. Remoção de stop-words\n",
    "6. Lemmatização\n",
    "\n",
    "Abaixo são realizadas demonstrações das duas execuções junto das métricas obtidas em sua execução:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo inicial - com pré-procesamento simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision: 0.972\n",
      "average recall: 0.910\n",
      "average f1: 0.940\n"
     ]
    }
   ],
   "source": [
    "INITIAL_PRE_PROCESS = True\n",
    "metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo melhorado - com pré-processamento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision: 0.957\n",
      "average recall: 0.942\n",
      "average f1: 0.949\n"
     ]
    }
   ],
   "source": [
    "INITIAL_PRE_PROCESS = False\n",
    "metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode-se observar, o modelo já apresentou uma boa precisão para classificação dos documentos (aproximadamente 94,0%), porém, a mesma pode ser melhorada após a realização do pipeline completo de pré-processamento, o que melhorou sua precisão em aproximadamente 0,9%, atingindo 94,9%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
